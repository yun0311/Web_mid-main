<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <header>
      <h1>Hello World</h1>
      <nav>
        <ul>
          <li><a href="index.html">홈</a></li>
          <li><a href="about.html">주제소개</a></li>
          <li><a href="tech.html">기술소개</a></li>
          <li><a href="team.html">팀 소개</a></li>
        </ul>
      </nav>
    </header>
    <header>
      <nav>
        <ul>
          <li><a href="about.html">초창기 ai</a></li>
          <li><a href="about2.html">'딥러닝'의 발전</a></li>
          <li><a href="about3.html">생성형 ai 붐</a></li>
        </ul>
      </nav>
    </header>
    <div>
      <h3 class="two-text">
        인공신경망 초기 연구는 1969년, 앞서 언급했던 퍼셉트론 모델이 비선형
        문제를 해결할 수 없다는 것*이 밝혀지면서 긴 침체기에 접어들었다. 이후,
        인공신경망 연구를 다시 수면위로 끌어올린 인물은 ‘딥러닝의 대부’로 불리는
        제프리 힌튼(Geoffrey Hinton, 이하 힌튼)이다. 1986년, 힌튼은 인공신경망을
        여러 겹 쌓은 다층 퍼셉트론(Multi-Layer Perceptrons) 이론에 역전파*
        알고리즘을 적용하여 퍼셉트론의 기존 문제를 해결할 수 있음을 증명했다.
        이를 계기로 인공신경망 연구가 다시 활기를 되찾는 듯했지만, 신경망의
        깊이가 깊어질수록 학습 과정과 결과에 이상이 나타나는 문제가 발생했다.
        2006년, 힌튼은 ‘A fast learning algorithm for deep belief nets’라는
        논문을 통해 다층 퍼셉트론의 성능을 높인 ‘심층 신뢰 신경망(Deep Belief
        Network, DBN)’을 제시했다. 심층 신뢰 신경망은 비지도학습*을 통해 각 층을
        사전 훈련한 후, 전체 네트워크를 미세 조정하는 방식으로 신경망의 학습
        속도와 효율성을 크게 높였다. 또한 AI 기술을 대표하는 알고리즘인
        ‘딥러닝(Deep Learnning)’의 기초 개념을 정립했다. 그리고 2012년, 딥러닝의
        압도적인 성능을 증명한 역사적인 사건이 발생한다. 바로 이미지 인식
        경진대회인 ILSVRC(ImageNet Large Scale Visual Recognition Challenge)에서
        힌튼이 이끄는 팀의 알렉스넷(AlexNet)이 우승을 차지한 것이다. 딥러닝
        기반의 모델인 알렉스넷은 이미지 인식률 84.7%를 기록했는데, 이는 다른
        모델과 비교했을 때 월등히 높은 수치였다. 특히, 전년도 우승팀의 오류율
        25.8%를 무려 16.4%까지 낮추는 기염을 토했다. AI 연구의 대세가 된
        딥러닝은 2010년대부터 급속도로 성장하게 되는데, 이러한 성장에는 두 가지
        배경이 있다. 첫째, GPU(Graphics Processing Unit, 그래픽처리장치)를
        비롯한 컴퓨터 시스템의 발전이다. GPU는 본래 컴퓨터의 그래픽을 처리하기
        위해 만들어졌다. CPU(Central Processing Unit, 중앙처리장치)와 비교할 때,
        GPU는 유사하고 반복적인 연산을 병렬로 처리하여 훨씬 속도가 빠르다.
        2010년대에 들어서며, GPU가 CPU의 역할까지 대신할 수 있게 보완한
        GPGPU(General-Purpose computing on GPU) 기술이 등장했다. GPU의 쓰임새가
        늘어났고, 인공신경망의 학습에도 적용되며 딥러닝의 발전이 크게
        가속화되었다. 방대한 학습 데이터를 분석해 특성을 추출하는 딥러닝은
        반복적인 계산이 필수인데, GPU의 병렬 계산 구조는 이에 아주 적합했기
        때문이다. 두 번째는 데이터(Data)의 증가다. 인공신경망 학습에는 대량의
        데이터가 필요하다. 과거의 데이터는 컴퓨터에 입력된 정보 수준에 그쳤다.
        하지만 1990년대 이후, 인터넷이 보급되고 검색엔진이 발전하며, 가공할 수
        있는 데이터의 범위가 기하급수적으로 늘어났다. 2000년대 이후에는
        스마트폰과 사물인터넷(Internet of Things, IoT)이 발전하며 빅데이터(Big
        Data)의 개념이 등장한다. 현실 세계의 곳곳에서 셀 수 없이 많은 데이터가
        실시간으로 수집되는 것이다. 많은 데이터를 학습한 딥러닝 알고리즘은 더욱
        정교하게 구축된다. 데이터 패러다임의 변화는 딥러닝 기술 발전의 큰 기반이
        되었다. 그리고 2016년, 딥러닝은 또 한 번 세상을 바꾼다. 구글 딥마인드가
        개발한 AI 알파고(AlphaGo)가 4승 1패로 바둑기사 이세돌 9단을 꺾으며 승리,
        전 세계에 AI의 존재를 각인한 것이다. 알파고는 딥러닝 알고리즘과
        강화학습*, 몬테카를로 트리 탐색* 알고리즘을 결합해 탄생했다. 이를 통해
        수만 번의 자가 대국을 진행하여 스스로 학습하고, 인간의 직관을 모방하여
        수를 예측하고 전략까지 세울 수 있었다. ‘인간을 꺾은 AI’의 탄생은
        본격적인 AI 시대의 시작을 알린 신호탄이었다
      </h3>

      <h5 class="three-text">
        * 초기 퍼셉트론 모델은 단층 퍼셉트론(Single-layer Perceptron)으로 입력
        값이 두 개이고, 두 입력 값이 같으면 0을 출력하고 다르면 1을 출력하는 XOR
        문제와 같은 비선형 문제를 해결할 수 없었다. * 역전파(Backpropagation):
        신경망에서 출력 값과 실제 값 사이의 차이를 계산하고, 오차를 줄이기 위해
        출력부터 시작하여 역순으로 가중치를 조절하는 알고리즘 * 비지도학습
        (Unsupervised Learning): 머신러닝의 학습론 중 하나로 입력 데이터에 대한
        정답을 주지 않고, 숨은 구조나 패턴 등을 발견하고 이해할 수 있게
        학습시키는 방법 * 강화학습(Reinforcement Learning): AI가 행동을 학습하는
        방식 중 하나. 행동에 따른 결과를 보상의 형태로 알려주면서, 주어진
        상태에서 최적의 행동을 선택하는 전략을 찾게 한다. * 몬테카를로 트리
        탐색(Monte Carlo tree search, MCTS): 일련의 난수를 반복적으로 생성하여
        함수의 값을 수리적으로 근사하는 확률적 알고리즘의 일종. 현 상황에서 선택
        가능한 행동들을 탐색 트리로 구조화하고, 무작위적 시뮬레이션을 통해 각
        행동의 득실을 추론하여 최적의 행동을 결정하는 기능을 한다.
      </h5>
      <img src="picture/image copy.png" alt="graff" class="about2-img" />
    </div>
  </body>
</html>
